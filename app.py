import streamlit as st

# Configure streamlit FIRST - must be before any other st commands
st.set_page_config(page_title="Advanced Sales Forecasting Dashboard", layout="wide")

import pandas as pd
import numpy as np
import logging
from datetime import datetime
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import io
from scipy import stats
from scipy.optimize import minimize

# Forecasting libraries
try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
except ImportError:
    PROPHET_AVAILABLE = False

try:
    from statsmodels.tsa.statespace.sarimax import SARIMAX
    from statsmodels.tsa.seasonal import seasonal_decompose
    from statsmodels.tsa.holtwinters import ExponentialSmoothing
    from statsmodels.stats.diagnostic import acorr_ljungbox
    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False

# Machine learning libraries
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import VotingRegressor
from sklearn.base import BaseEstimator, RegressorMixin

import warnings
warnings.filterwarnings("ignore")

# Global variable for the default adjustment factor
FORECAST_ADJUSTMENT_FACTOR = 0.85  # 85% = 100% - 15% reduction


def apply_forecast_adjustment(forecast_values, adjustment_factor=None):
    """
    Apply adjustment factor to forecast values.
    
    Args:
        forecast_values: Array of forecast values
        adjustment_factor: Factor to multiply forecasts by (if None, uses global default)
    
    Returns:
        Adjusted forecast values
    """
    if adjustment_factor is None:
        adjustment_factor = FORECAST_ADJUSTMENT_FACTOR
        
    if isinstance(forecast_values, (list, np.ndarray)):
        adjusted_values = np.array(forecast_values) * adjustment_factor
        return np.maximum(adjusted_values, 0)  # Ensure non-negative values
    return forecast_values


class MetaLearner(BaseEstimator, RegressorMixin):
    """Meta-learner for model stacking"""
    def __init__(self):
        self.meta_model = Ridge(alpha=1.0)
        
    def fit(self, X, y):
        self.meta_model.fit(X, y)
        return self
    
    def predict(self, X):
        return self.meta_model.predict(X)


@st.cache_data
def load_data(uploaded_file):
    """
    Load and preprocess the historical sales data with advanced preprocessing.
    Properly aggregates multiple entries per month.
    """
    try:
        df = pd.read_excel(uploaded_file)
    except Exception:
        st.error("Could not read the uploaded file. Please ensure it's a valid Excel file.")
        return None

    if "Month" not in df.columns or "Sales" not in df.columns:
        st.error("The file must contain 'Month' and 'Sales' columns.")
        return None

    # Parse dates
    df["Month"] = pd.to_datetime(df["Month"], errors="coerce")
    if df["Month"].isna().any():
        st.error("Some dates could not be parsed. Please check the 'Month' column format.")
        return None

    # Clean sales data
    df["Sales"] = pd.to_numeric(df["Sales"], errors="coerce").fillna(0)
    df["Sales"] = df["Sales"].abs()

    # Sort by date
    df = df.sort_values("Month").reset_index(drop=True)
    
    # Check if there are multiple entries per month
    original_rows = len(df)
    unique_months = df['Month'].nunique()
    
    if original_rows > unique_months:
        st.info(f"üìä Aggregating {original_rows} data points into {unique_months} monthly totals...")
        
        # Aggregate by month - sum all sales for each month
        df_monthly = df.groupby('Month', as_index=False).agg({
            'Sales': 'sum'  # Sum all sales for each month
        }).sort_values('Month').reset_index(drop=True)
        
        # Add original sales column for reference
        df_monthly['Sales_Original'] = df_monthly['Sales'].copy()
        
        # Advanced preprocessing on the monthly aggregated data
        df_processed = preprocess_data(df_monthly)
        
        st.success(f"‚úÖ Successfully aggregated to {len(df_processed)} monthly data points")
        
    else:
        # Data is already monthly, just preprocess
        df_processed = preprocess_data(df)
    
    return df_processed[["Month", "Sales", "Sales_Original"]]


def preprocess_data(df):
    """
    Advanced data preprocessing for improved accuracy.
    """
    # Store original sales for reference
    df['Sales_Original'] = df['Sales'].copy()
    
    # 1. Outlier Detection and Treatment using IQR
    Q1 = df['Sales'].quantile(0.25)
    Q3 = df['Sales'].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Cap outliers instead of removing (preserves data points)
    outliers_detected = ((df['Sales'] < lower_bound) | (df['Sales'] > upper_bound)).sum()
    if outliers_detected > 0:
        st.info(f"üìä Detected and capped {outliers_detected} outliers for better model stability")
        df['Sales'] = df['Sales'].clip(lower=lower_bound, upper=upper_bound)
    
    # 2. Handle missing values with interpolation
    if df['Sales'].isna().any():
        df['Sales'] = df['Sales'].interpolate(method='time')
    
    # 3. Data transformation - test for optimal transformation
    skewness = stats.skew(df['Sales'])
    if abs(skewness) > 1:  # Highly skewed data
        st.info(f"üìà Data skewness detected ({skewness:.2f}). Applying log transformation for better modeling.")
        df['Sales'] = np.log1p(df['Sales'])  # log1p handles zeros better
        df['log_transformed'] = True
    else:
        df['log_transformed'] = False
    
    return df


@st.cache_data
def load_actual_2024_data(uploaded_file, forecast_year):
    """
    Load actual data with preprocessing - only include months that have actual data
    """
    try:
        df = pd.read_excel(uploaded_file)
        
        # Check if it's the standard long format
        if "Month" in df.columns and "Sales" in df.columns:
            df["Month"] = pd.to_datetime(df["Month"], errors="coerce")
            if df["Month"].isna().any():
                st.error("Some dates in the actual file could not be parsed.")
                return None

            df["Sales"] = pd.to_numeric(df["Sales"], errors="coerce").fillna(0)
            df["Sales"] = df["Sales"].abs()

            # Filter to the forecast year only
            start = pd.Timestamp(f"{forecast_year}-01-01")
            end = pd.Timestamp(f"{forecast_year+1}-01-01")
            df = df[(df["Month"] >= start) & (df["Month"] < end)]
            
            if df.empty:
                st.warning(f"No rows match year {forecast_year}.")
                return None

            # Only include months that have actual non-zero data
            monthly = df.groupby("Month", as_index=False)["Sales"].sum()
            monthly = monthly[monthly["Sales"] > 0]  # Only months with actual sales
            monthly = monthly.sort_values("Month").reset_index(drop=True)
            
            return monthly.rename(columns={"Sales": f"Actual_{forecast_year}"})
        
        else:
            # Wide format handling
            st.info("üìä Detected wide format data - converting to long format...")
            
            month_patterns = [
                f"Jan-{forecast_year}", f"Feb-{forecast_year}", f"Mar-{forecast_year}",
                f"Apr-{forecast_year}", f"May-{forecast_year}", f"Jun-{forecast_year}",
                f"Jul-{forecast_year}", f"Aug-{forecast_year}", f"Sep-{forecast_year}",
                f"Oct-{forecast_year}", f"Nov-{forecast_year}", f"Dec-{forecast_year}"
            ]
            
            # Only include month patterns that actually exist in the data
            available_months = [pattern for pattern in month_patterns if pattern in df.columns]
            
            if not available_months:
                st.error(f"No month columns found for {forecast_year}.")
                return None
            
            st.info(f"üìÖ Found data for months: {', '.join([m.split('-')[0] for m in available_months])}")
            
            first_col = df.columns[0]
            data_rows = df[~df[first_col].astype(str).str.contains("Item|Code|QTY", case=False, na=False)]
            
            melted_data = []
            
            for _, row in data_rows.iterrows():
                for month_col in available_months:
                    if month_col in row and pd.notna(row[month_col]):
                        sales_value = pd.to_numeric(row[month_col], errors="coerce")
                        if pd.notna(sales_value) and sales_value > 0:
                            month_str = month_col.replace("-", "-01-")
                            try:
                                month_date = pd.to_datetime(month_str, format="%b-%d-%Y")
                                melted_data.append({
                                    "Month": month_date,
                                    "Sales": abs(sales_value)
                                })
                            except:
                                continue
            
            if not melted_data:
                st.error("No valid sales data found.")
                return None
            
            long_df = pd.DataFrame(melted_data)
            
            # Group by month and sum, but only for months that actually have data
            monthly = long_df.groupby("Month", as_index=False)["Sales"].sum()
            monthly = monthly[monthly["Sales"] > 0]  # Only months with actual sales data
            monthly = monthly.sort_values("Month").reset_index(drop=True)
            
            # Show which months were actually processed
            processed_months = monthly['Month'].dt.strftime('%b').tolist()
            st.success(f"‚úÖ Successfully processed data for: {', '.join(processed_months)}")
            
            return monthly.rename(columns={"Sales": f"Actual_{forecast_year}"})
            
    except Exception as e:
        st.error(f"Error loading actual data: {str(e)}")
        return None


def calculate_accuracy_metrics(actual, forecast):
    """Enhanced accuracy metrics"""
    if len(actual) == 0 or len(forecast) == 0:
        return None
    
    mask = ~(pd.isna(actual) | pd.isna(forecast))
    actual_clean = actual[mask]
    forecast_clean = forecast[mask]
    
    if len(actual_clean) == 0:
        return None
    
    # Standard metrics
    mape = np.mean(np.abs((actual_clean - forecast_clean) / actual_clean)) * 100
    mae = mean_absolute_error(actual_clean, forecast_clean)
    rmse = np.sqrt(mean_squared_error(actual_clean, forecast_clean))
    
    # Additional metrics
    smape = np.mean(2 * np.abs(forecast_clean - actual_clean) / (np.abs(actual_clean) + np.abs(forecast_clean))) * 100
    mase = mae / np.mean(np.abs(np.diff(actual_clean))) if len(actual_clean) > 1 else mae
    
    return {
        "MAPE": mape,
        "MAE": mae,
        "RMSE": rmse,
        "SMAPE": smape,
        "MASE": mase
    }


def detect_and_apply_scaling(historical_data, actual_data=None):
    """Enhanced scaling detection with multiple methods"""
    hist_avg = historical_data['Sales'].mean()
    
    if actual_data is not None and len(actual_data) > 0:
        actual_avg = actual_data.iloc[:, 1].mean()
        
        # Multiple scaling detection methods
        ratio = actual_avg / hist_avg if hist_avg > 0 else 1
        
        # Apply scaling if ratio is significant
        if ratio > 2 or ratio < 0.5:
            st.warning(f"üìä Scale mismatch detected! Scaling factor: {ratio:.2f}")
            return ratio
    
    return 1.0


def optimize_sarima_parameters(data, max_p=2, max_d=2, max_q=2, seasonal_periods=12):
    """Optimize SARIMA parameters using grid search - more conservative approach"""
    if not STATSMODELS_AVAILABLE:
        return {'order': (1, 1, 1), 'seasonal_order': (1, 1, 1, 12)}
    
    best_aic = np.inf
    best_params = None
    
    # More conservative grid search for stability
    param_combinations = [
        ((1, 1, 1), (1, 1, 1, 12)),
        ((0, 1, 1), (0, 1, 1, 12)),
        ((1, 0, 1), (1, 0, 1, 12)),
        ((2, 1, 0), (1, 1, 0, 12)),
        ((0, 1, 2), (0, 1, 1, 12)),
        ((1, 1, 0), (0, 1, 1, 12))
    ]
    
    for order, seasonal_order in param_combinations:
        try:
            model = SARIMAX(
                data['Sales'],
                order=order,
                seasonal_order=seasonal_order,
                enforce_stationarity=False,
                enforce_invertibility=False
            )
            fitted = model.fit(disp=False, maxiter=100, method='lbfgs')
            
            if fitted.aic < best_aic and np.isfinite(fitted.aic):
                best_aic = fitted.aic
                best_params = {
                    'order': order,
                    'seasonal_order': seasonal_order
                }
        except Exception as e:
            continue
    
    return best_params if best_params else {'order': (1, 1, 1), 'seasonal_order': (1, 1, 1, 12)}


def run_advanced_sarima_forecast(data, forecast_periods=12, scaling_factor=1.0, adjustment_factor=None):
    """Fixed SARIMA with better error handling and validation"""
    try:
        if not STATSMODELS_AVAILABLE:
            forecast_values = run_fallback_forecast(data, forecast_periods, scaling_factor, adjustment_factor)
            return apply_forecast_adjustment(forecast_values, adjustment_factor), np.inf
        
        # Ensure we have enough data points
        if len(data) < 24:
            st.warning("‚ö†Ô∏è SARIMA needs at least 24 data points. Using fallback method.")
            forecast_values = run_fallback_forecast(data, forecast_periods, scaling_factor, adjustment_factor)
            return apply_forecast_adjustment(forecast_values, adjustment_factor), np.inf
        
        # Create a copy to avoid modifying original data
        work_data = data.copy()
        
        # Check if data was log transformed
        log_transformed = 'log_transformed' in work_data.columns and work_data['log_transformed'].iloc[0]
        
        # Ensure data is stationary and has positive values
        sales_series = work_data['Sales'].copy()
        
        # Check for zeros or negative values that could cause issues
        if (sales_series <= 0).any():
            sales_series = sales_series.clip(lower=0.1)  # Replace zeros/negatives with small positive value
        
        # Optimize parameters with conservative approach
        with st.spinner("üîß Optimizing SARIMA parameters..."):
            best_params = optimize_sarima_parameters(work_data)
        
        # Fit the model with additional error handling
        model = SARIMAX(
            sales_series, 
            order=best_params['order'],
            seasonal_order=best_params['seasonal_order'],
            enforce_stationarity=False,
            enforce_invertibility=False
        )
        
        # Fit with multiple methods if first fails
        fitted_model = None
        for method in ['lbfgs', 'bfgs', 'nm']:
            try:
                fitted_model = model.fit(
                    disp=False, 
                    maxiter=200, 
                    method=method,
                    low_memory=True
                )
                break
            except Exception:
                continue
        
        if fitted_model is None:
            raise ValueError("All fitting methods failed")
        
        # Generate forecast with confidence intervals
        forecast_result = fitted_model.get_forecast(steps=forecast_periods)
        forecast = forecast_result.predicted_mean
        
        # Validate forecast results
        if not isinstance(forecast, (pd.Series, np.ndarray)) or len(forecast) != forecast_periods:
            raise ValueError("Invalid forecast format or length")
        
        # Convert to numpy array and ensure proper format
        forecast_values = np.array(forecast)
        
        # Check for invalid values
        if np.any(np.isnan(forecast_values)) or np.any(np.isinf(forecast_values)):
            raise ValueError("Forecast contains NaN or infinite values")
        
        # Reverse log transformation first if applied
        if log_transformed:
            forecast_values = np.expm1(forecast_values)
        
        # Apply scaling and ensure positive values
        forecast_values = np.maximum(forecast_values, 0) * scaling_factor
        
        # Final validation
        if len(forecast_values) != 12:
            raise ValueError(f"Expected 12 forecast values, got {len(forecast_values)}")
        
        # Apply adjustment
        return apply_forecast_adjustment(forecast_values, adjustment_factor), fitted_model.aic
        
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Advanced SARIMA failed: {str(e)}. Using fallback method.")
        forecast_values = run_fallback_forecast(data, forecast_periods, scaling_factor, adjustment_factor)
        return apply_forecast_adjustment(forecast_values, adjustment_factor), np.inf


def run_advanced_prophet_forecast(data, forecast_periods=12, scaling_factor=1.0, adjustment_factor=None):
    """Enhanced Prophet with better error handling"""
    try:
        if not PROPHET_AVAILABLE:
            forecast_values = run_fallback_forecast(data, forecast_periods, scaling_factor, adjustment_factor)
            return apply_forecast_adjustment(forecast_values, adjustment_factor), np.inf
        
        # Create a copy to avoid modifying original data
        work_data = data.copy()
        
        # Check if data was log transformed
        log_transformed = 'log_transformed' in work_data.columns and work_data['log_transformed'].iloc[0]
        
        # Prepare data for Prophet
        prophet_data = work_data[['Month', 'Sales']].rename(columns={'Month': 'ds', 'Sales': 'y'})
        
        # Ensure positive values for Prophet
        prophet_data['y'] = prophet_data['y'].clip(lower=0.1)
        
        # Use simpler Prophet configuration for stability
        model = Prophet(
            yearly_seasonality=True,
            weekly_seasonality=False,
            daily_seasonality=False,
            seasonality_mode='additive',
            changepoint_prior_scale=0.05,
            seasonality_prior_scale=10.0
        )
        
        model.fit(prophet_data)
        
        # Create future dataframe
        future = model.make_future_dataframe(periods=forecast_periods, freq='MS')
        forecast = model.predict(future)
        
        # Extract forecast values
        forecast_values = forecast['yhat'].tail(forecast_periods).values
        
        # Validate forecast
        if len(forecast_values) != forecast_periods:
            raise ValueError(f"Expected {forecast_periods} forecast values, got {len(forecast_values)}")
        
        # Reverse log transformation first if applied
        if log_transformed:
            forecast_values = np.expm1(forecast_values)
        
        # Apply scaling and ensure positive values
        forecast_values = np.maximum(forecast_values, 0) * scaling_factor
        
        # Apply adjustment
        return apply_forecast_adjustment(forecast_values, adjustment_factor), np.mean(np.abs(forecast['yhat'] - prophet_data['y']))
        
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Advanced Prophet failed: {str(e)}. Using fallback method.")
        forecast_values = run_fallback_forecast(data, forecast_periods, scaling_factor, adjustment_factor)
        return apply_forecast_adjustment(forecast_values, adjustment_factor), np.inf


def run_advanced_ets_forecast(data, forecast_periods=12, scaling_factor=1.0, adjustment_factor=None):
    """Advanced ETS with better error handling"""
    try:
        if not STATSMODELS_AVAILABLE:
            forecast_values = run_fallback_forecast(data, forecast_periods, scaling_factor, adjustment_factor)
            return apply_forecast_adjustment(forecast_values, adjustment_factor), np.inf
        
        # Create a copy to avoid modifying original data
        work_data = data.copy()
        
        # Check if data was log transformed
        log_transformed = 'log_transformed' in work_data.columns and work_data['log_transformed'].iloc[0]
        
        # Ensure positive values
        sales_series = work_data['Sales'].clip(lower=0.1)
        
        # Try simple additive model first
        try:
            model = ExponentialSmoothing(
                sales_series,
                seasonal='add',
                seasonal_periods=12,
                trend='add'
            )
            fitted_model = model.fit(optimized=True)
            forecast = fitted_model.forecast(steps=forecast_periods)
            
        except Exception:
            # Fallback to simpler model
            model = ExponentialSmoothing(
                sales_series,
                seasonal=None,
                trend='add'
            )
            fitted_model = model.fit(optimized=True)
            forecast = fitted_model.forecast(steps=forecast_periods)
        
        # Validate forecast
        forecast_values = np.array(forecast)
        if len(forecast_values) != forecast_periods:
            raise ValueError(f"Expected {forecast_periods} forecast values, got {len(forecast_values)}")
        
        # Reverse log transformation first if applied
        if log_transformed:
            forecast_values = np.expm1(forecast_values)
        
        # Apply scaling and ensure positive values
        forecast_values = np.maximum(forecast_values, 0) * scaling_factor
        
        # Apply adjustment
        return apply_forecast_adjustment(forecast_values, adjustment_factor), fitted_model.aic
        
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Advanced ETS failed: {str(e)}. Using fallback method.")
        forecast_values = run_fallback_forecast(data, forecast_periods, scaling_factor, adjustment_factor)
        return apply_forecast_adjustment(forecast_values, adjustment_factor), np.inf


def run_advanced_xgb_forecast(data, forecast_periods=12, scaling_factor=1.0, adjustment_factor=None):
    """Simplified XGBoost forecast with better error handling"""
    try:
        # Create a copy to avoid modifying original data
        work_data = data.copy()
        
        # Check if data was log transformed
        log_transformed = 'log_transformed' in work_data.columns and work_data['log_transformed'].iloc[0]
        
        # Simple feature-based approach
        if len(work_data) >= 12:
            # Use last 12 months as seasonal pattern
            recent_sales = work_data['Sales'].tail(12).values
            
            # Calculate trend
            trend = np.polyfit(range(len(recent_sales)), recent_sales, 1)[0]
            
            # Generate forecasts with seasonal pattern and trend
            forecasts = []
            for i in range(forecast_periods):
                month_idx = i % 12
                seasonal_base = recent_sales[month_idx] if month_idx < len(recent_sales) else np.mean(recent_sales)
                trend_adjustment = trend * (i + 1)
                forecast_val = max(seasonal_base + trend_adjustment * 0.5, seasonal_base * 0.8)
                forecasts.append(forecast_val)
        else:
            # Fallback for insufficient data
            base_value = work_data['Sales'].mean()
            forecasts = [base_value] * forecast_periods
        
        forecasts = np.array(forecasts)
        
        # Validate forecast
        if len(forecasts) != forecast_periods:
            raise ValueError(f"Expected {forecast_periods} forecast values, got {len(forecasts)}")
        
        # Reverse log transformation first if applied
        if log_transformed:
            forecasts = np.expm1(forecasts)
        
        # Apply scaling and ensure positive values
        forecasts = np.maximum(forecasts, 0) * scaling_factor
        
        # Apply adjustment
        return apply_forecast_adjustment(forecasts, adjustment_factor), 200.0
        
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Advanced XGBoost failed: {str(e)}. Using fallback method.")
        forecast_values = run_fallback_forecast(data, forecast_periods, scaling_factor, adjustment_factor)
        return apply_forecast_adjustment(forecast_values, adjustment_factor), np.inf


def run_fallback_forecast(data, forecast_periods=12, scaling_factor=1.0, adjustment_factor=None):
    """Robust fallback forecasting method"""
    try:
        # Create a copy to avoid modifying original data
        work_data = data.copy()
        
        # Check if data was log transformed
        log_transformed = 'log_transformed' in work_data.columns and work_data['log_transformed'].iloc[0]
        
        if len(work_data) >= 12:
            # Use seasonal naive with trend
            seasonal_pattern = work_data['Sales'].tail(12).values
            recent_trend = np.polyfit(range(len(work_data['Sales'].tail(12))), work_data['Sales'].tail(12), 1)[0]
            
            forecast = []
            for i in range(forecast_periods):
                seasonal_val = seasonal_pattern[i % 12]
                trend_adjustment = recent_trend * (i + 1) * 0.5  # Dampen trend
                forecast_val = max(seasonal_val + trend_adjustment, seasonal_val * 0.7)
                forecast.append(forecast_val)
            
            forecast = np.array(forecast)
            
            # Reverse log transformation first if applied
            if log_transformed:
                forecast = np.expm1(forecast)
            
            # Apply scaling
            forecast = forecast * scaling_factor
            
            return forecast
        else:
            base_forecast = work_data['Sales'].mean()
            
            # Reverse log transformation first if applied
            if log_transformed:
                base_forecast = np.expm1(base_forecast)
            
            # Apply scaling
            base_forecast = base_forecast * scaling_factor
            
            return np.array([base_forecast] * forecast_periods)
            
    except Exception as e:
        # Ultimate fallback - use historical mean
        try:
            historical_mean = data['Sales'].mean() if len(data) > 0 else 1000
            return np.array([historical_mean * scaling_factor] * forecast_periods)
        except:
            return np.array([1000 * scaling_factor] * forecast_periods)


def create_weighted_ensemble(forecasts_dict, validation_scores):
    """Create weighted ensemble based on validation performance"""
    # Convert scores to weights (inverse of error - lower error = higher weight)
    weights = {}
    total_inverse_score = 0
    
    for model_name, score in validation_scores.items():
        if score != np.inf and score > 0:
            inverse_score = 1 / score
            weights[model_name] = inverse_score
            total_inverse_score += inverse_score
        else:
            weights[model_name] = 0.1  # Small weight for failed models
            total_inverse_score += 0.1
    
    # Normalize weights
    for model_name in weights:
        weights[model_name] = weights[model_name] / total_inverse_score
    
    # Create weighted ensemble
    ensemble_forecast = np.zeros(len(next(iter(forecasts_dict.values()))))
    
    for model_name, forecast in forecasts_dict.items():
        model_key = model_name.replace('_Forecast', '')
        weight = weights.get(model_key, 0.25)  # Default equal weight if not found
        ensemble_forecast += weight * forecast
    
    # Since individual forecasts are already adjusted, we don't need to adjust again
    return ensemble_forecast, weights


def run_meta_learning_forecast(forecasts_dict, actual_data=None, forecast_periods=12):
    """Advanced meta-learning ensemble using stacking"""
    if actual_data is None or len(actual_data) < 12:
        # Fallback to simple ensemble if no validation data
        return None
    
    try:
        # Simple average of all forecasts for now
        forecast_values = list(forecasts_dict.values())
        meta_forecast = np.mean(forecast_values, axis=0)
        # Since individual forecasts are already adjusted, we don't need to adjust again
        return np.maximum(meta_forecast, 0)
    
    except Exception as e:
        return None


def create_comparison_chart_for_available_months_only(result_df, forecast_year, adjustment_percentage):
    """
    Create comparison chart only for months where actual data exists
    """
    actual_col = f'Actual_{forecast_year}'
    
    if actual_col not in result_df.columns:
        return None
    
    # Filter to only months that have actual data
    available_data = result_df[result_df[actual_col].notna()].copy()
    
    if len(available_data) == 0:
        return None
    
    # Also filter forecast data to the same months for fair comparison
    forecast_cols = [col for col in result_df.columns if '_Forecast' in col or col in ['Weighted_Ensemble', 'Meta_Learning']]
    
    fig = go.Figure()
    
    # Add actual data
    fig.add_trace(go.Scatter(
        x=available_data['Month'],
        y=available_data[actual_col],
        mode='lines+markers',
        name='üéØ ACTUAL',
        line=dict(color='#FF6B6B', width=4),
        marker=dict(size=12, symbol='circle')
    ))
    
    # Add forecast data for the same months
    colors = ['#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#FF9F43', '#6C5CE7']
    for i, col in enumerate(forecast_cols):
        if col in ['Weighted_Ensemble', 'Meta_Learning']:
            line_style = dict(color='#6C5CE7', width=3, dash='dash') if col == 'Weighted_Ensemble' else dict(color='#00D2D3', width=3, dash='dot')
            icon = 'üî•' if col == 'Weighted_Ensemble' else 'üß†'
        else:
            line_style = dict(color=colors[i % len(colors)], width=2)
            icon = 'üìà'
        
        model_name = col.replace('_Forecast', '').replace('_', ' ').upper()
        fig.add_trace(go.Scatter(
            x=available_data['Month'],
            y=available_data[col],
            mode='lines+markers',
            name=f'{icon} {model_name}',
            line=line_style,
            marker=dict(size=6)
        ))
    
    # Show available months in title
    month_names = available_data['Month'].dt.strftime('%b').tolist()
    months_text = ', '.join(month_names)
    
    # Create dynamic title based on adjustment
    if adjustment_percentage < 0:
        adj_text = f"{abs(adjustment_percentage):.1f}% Reduction Applied"
    elif adjustment_percentage > 0:
        adj_text = f"{adjustment_percentage:.1f}% Increase Applied"
    else:
        adj_text = "No Adjustment Applied"
    
    fig.update_layout(
        title=f'üöÄ ADVANCED AI MODELS vs ACTUAL PERFORMANCE ({adj_text})<br><sub>Comparison for available months: {months_text}</sub>',
        xaxis_title='Month',
        yaxis_title='Sales Volume',
        height=700,
        hovermode='x unified',
        showlegend=True,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
    )
    
    return fig


def create_historical_sales_comparison_chart(hist_df):
    """
    Create a chart showing historical sales comparison across all years
    """
    try:
        # Create a copy and ensure we have the right data
        work_data = hist_df.copy()
        
        # Ensure Month is datetime
        work_data['Month'] = pd.to_datetime(work_data['Month'])
        
        # Use original sales data if available, otherwise use processed data
        sales_col = 'Sales_Original' if 'Sales_Original' in work_data.columns else 'Sales'
        
        # Extract year and month for grouping
        work_data['Year'] = work_data['Month'].dt.year
        work_data['Month_Name'] = work_data['Month'].dt.strftime('%b')
        work_data['Month_Num'] = work_data['Month'].dt.month
        
        # Aggregate sales by month (sum all entries for each month)
        monthly_data = work_data.groupby('Month', as_index=False).agg({
            sales_col: 'sum',
            'Year': 'first',
            'Month_Name': 'first', 
            'Month_Num': 'first'
        }).rename(columns={sales_col: 'Sales'})
        
        # Get unique years
        years = sorted(monthly_data['Year'].unique())
        
        if len(years) < 2:
            # If only one year, show monthly progression
            fig = go.Figure()
            
            year_data = monthly_data[monthly_data['Year'] == years[0]]
            year_data = year_data.sort_values('Month_Num')
            
            fig.add_trace(go.Scatter(
                x=year_data['Month_Name'],
                y=year_data['Sales'],
                mode='lines+markers',
                name=f'{years[0]}',
                line=dict(width=3),
                marker=dict(size=8)
            ))
            
            fig.update_layout(
                title=f'üìä Historical Sales Data - {years[0]}',
                xaxis_title='Month',
                yaxis_title='Sales Volume',
                height=500,
                hovermode='x unified'
            )
        
        else:
            # Multiple years - show comparison
            fig = go.Figure()
            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#FF9F43', '#6C5CE7']
            
            for i, year in enumerate(years):
                year_data = monthly_data[monthly_data['Year'] == year]
                year_data = year_data.sort_values('Month_Num')
                
                # Calculate year total for legend
                year_total = year_data['Sales'].sum()
                
                fig.add_trace(go.Scatter(
                    x=year_data['Month_Name'],
                    y=year_data['Sales'],
                    mode='lines+markers',
                    name=f'{year} (Total: {year_total:,.0f})',
                    line=dict(color=colors[i % len(colors)], width=3),
                    marker=dict(size=8),
                    hovertemplate=f'<b>{year}</b><br>Month: %{{x}}<br>Sales: %{{y:,.0f}}<extra></extra>'
                ))
            
            # Add year-over-year growth analysis
            if len(years) >= 2:
                year_totals = []
                for year in years:
                    year_total = monthly_data[monthly_data['Year'] == year]['Sales'].sum()
                    year_totals.append(year_total)
                
                # Calculate growth rates
                growth_info = []
                for i in range(1, len(years)):
                    growth_rate = ((year_totals[i] - year_totals[i-1]) / year_totals[i-1]) * 100
                    growth_info.append(f"{years[i-1]} to {years[i]}: {growth_rate:+.1f}%")
                
                growth_text = "<br>".join(growth_info)
                
                fig.update_layout(
                    title=f'üìä Historical Sales Comparison Across Years<br><sub>Year-over-Year Growth: {growth_text}</sub>',
                    xaxis_title='Month',
                    yaxis_title='Sales Volume',
                    height=600,
                    hovermode='x unified',
                    legend=dict(
                        orientation="v",
                        yanchor="top",
                        y=1,
                        xanchor="left",
                        x=1.01
                    )
                )
            else:
                fig.update_layout(
                    title='üìä Historical Sales Comparison Across Years',
                    xaxis_title='Month',
                    yaxis_title='Sales Volume',
                    height=600,
                    hovermode='x unified'
                )
        
        # Customize the chart
        fig.update_xaxes(
            tickmode='array',
            tickvals=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
        )
        
        fig.update_layout(
            showlegend=True,
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            font=dict(size=12)
        )
        
        return fig
        
    except Exception as e:
        st.error(f"Error creating historical comparison chart: {str(e)}")
        return None


def create_yearly_summary_table(hist_df):
    """
    Create a summary table showing key metrics by year
    """
    try:
        # Create a copy and ensure we have the right data
        work_data = hist_df.copy()
        
        # Ensure Month is datetime
        work_data['Month'] = pd.to_datetime(work_data['Month'])
        
        # Use original sales data if available
        sales_col = 'Sales_Original' if 'Sales_Original' in work_data.columns else 'Sales'
        
        # Extract year
        work_data['Year'] = work_data['Month'].dt.year
        
        # Aggregate by month first (sum multiple entries per month)
        monthly_data = work_data.groupby(['Year', work_data['Month'].dt.to_period('M')], as_index=False).agg({
            sales_col: 'sum'
        }).rename(columns={sales_col: 'Sales'})
        
        # Then aggregate by year
        yearly_summary = monthly_data.groupby('Year').agg({
            'Sales': ['sum', 'mean', 'std', 'count']
        }).round(0)
        
        # Flatten column names
        yearly_summary.columns = ['Total_Sales', 'Avg_Monthly', 'Std_Dev', 'Months_Data']
        yearly_summary = yearly_summary.reset_index()
        
        # Calculate year-over-year growth
        yearly_summary['YoY_Growth'] = yearly_summary['Total_Sales'].pct_change() * 100
        
        # Format for display
        display_summary = yearly_summary.copy()
        display_summary['Total_Sales'] = display_summary['Total_Sales'].apply(lambda x: f"{x:,.0f}")
        display_summary['Avg_Monthly'] = display_summary['Avg_Monthly'].apply(lambda x: f"{x:,.0f}")
        display_summary['Std_Dev'] = display_summary['Std_Dev'].apply(lambda x: f"{x:,.0f}")
        display_summary['YoY_Growth'] = display_summary['YoY_Growth'].apply(
            lambda x: f"{x:+.1f}%" if not pd.isna(x) else "N/A"
        )
        
        # Rename columns for better display
        display_summary = display_summary.rename(columns={
            'Year': 'Year',
            'Total_Sales': 'Total Annual Sales',
            'Avg_Monthly': 'Avg Monthly Sales', 
            'Std_Dev': 'Monthly Std Dev',
            'Months_Data': 'Months of Data',
            'YoY_Growth': 'YoY Growth (%)'
        })
        
        return display_summary
        
    except Exception as e:
        st.error(f"Error creating yearly summary: {str(e)}")
        return None
    """
    Create comparison chart only for months where actual data exists
    """
    actual_col = f'Actual_{forecast_year}'
    
    if actual_col not in result_df.columns:
        return None
    
    # Filter to only months that have actual data
    available_data = result_df[result_df[actual_col].notna()].copy()
    
    if len(available_data) == 0:
        return None
    
    # Also filter forecast data to the same months for fair comparison
    forecast_cols = [col for col in result_df.columns if '_Forecast' in col or col in ['Weighted_Ensemble', 'Meta_Learning']]
    
    fig = go.Figure()
    
    # Add actual data
    fig.add_trace(go.Scatter(
        x=available_data['Month'],
        y=available_data[actual_col],
        mode='lines+markers',
        name='üéØ ACTUAL',
        line=dict(color='#FF6B6B', width=4),
        marker=dict(size=12, symbol='circle')
    ))
    
    # Add forecast data for the same months
    colors = ['#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#FF9F43', '#6C5CE7']
    for i, col in enumerate(forecast_cols):
        if col in ['Weighted_Ensemble', 'Meta_Learning']:
            line_style = dict(color='#6C5CE7', width=3, dash='dash') if col == 'Weighted_Ensemble' else dict(color='#00D2D3', width=3, dash='dot')
            icon = 'üî•' if col == 'Weighted_Ensemble' else 'üß†'
        else:
            line_style = dict(color=colors[i % len(colors)], width=2)
            icon = 'üìà'
        
        model_name = col.replace('_Forecast', '').replace('_', ' ').upper()
        fig.add_trace(go.Scatter(
            x=available_data['Month'],
            y=available_data[col],
            mode='lines+markers',
            name=f'{icon} {model_name}',
            line=line_style,
            marker=dict(size=6)
        ))
    
    # Show available months in title
    month_names = available_data['Month'].dt.strftime('%b').tolist()
    months_text = ', '.join(month_names)
    
    # Create dynamic title based on adjustment
    if adjustment_percentage < 0:
        adj_text = f"{abs(adjustment_percentage):.1f}% Reduction Applied"
    elif adjustment_percentage > 0:
        adj_text = f"{adjustment_percentage:.1f}% Increase Applied"
    else:
        adj_text = "No Adjustment Applied"
    
    fig.update_layout(
        title=f'üöÄ ADVANCED AI MODELS vs ACTUAL PERFORMANCE ({adj_text})<br><sub>Comparison for available months: {months_text}</sub>',
        xaxis_title='Month',
        yaxis_title='Sales Volume',
        height=700,
        hovermode='x unified',
        showlegend=True,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
    )
    
    return fig


def main():
    """
    Main function to run the advanced forecasting app.
    """
    st.title("üöÄ Advanced AI Sales Forecasting Dashboard")
    st.markdown("**Next-generation forecasting with ML optimization, ensemble weighting, and meta-learning**")

    # Sidebar configuration
    st.sidebar.header("‚öôÔ∏è Advanced Configuration")
    forecast_year = st.sidebar.selectbox(
        "Select forecast year:",
        options=[2024, 2025, 2026],
        index=0
    )

    # Sidebar option to adjust forecast
    st.sidebar.subheader("üìä Forecast Adjustment")
    
    # Choice between slider and custom input
    adjustment_mode = st.sidebar.radio(
        "Adjustment Mode:",
        ["Slider (Quick)", "Custom Input (Precise)"],
        help="Choose how to set the forecast adjustment percentage"
    )
    
    if adjustment_mode == "Slider (Quick)":
        adjustment_percentage = st.sidebar.slider(
            "Forecast Adjustment (%)",
            min_value=-50,
            max_value=50,
            value=-15,  # Default 15% reduction
            step=5,
            help="Negative values reduce forecasts, positive values increase them"
        )
    else:  # Custom Input
        adjustment_percentage = st.sidebar.number_input(
            "Custom Forecast Adjustment (%)",
            min_value=-100.0,
            max_value=200.0,
            value=-15.0,
            step=0.1,
            format="%.1f",
            help="Enter any percentage: negative reduces, positive increases forecasts"
        )
    
    # Calculate current adjustment factor
    current_adjustment_factor = (100 + adjustment_percentage) / 100
    
    # Show interpretation
    if adjustment_percentage < 0:
        st.sidebar.error(f"üìâ **Reduction**: {abs(adjustment_percentage):.1f}% decrease (factor: {current_adjustment_factor:.3f}x)")
    elif adjustment_percentage > 0:
        st.sidebar.success(f"üìà **Increase**: {adjustment_percentage:.1f}% increase (factor: {current_adjustment_factor:.3f}x)")
    else:
        st.sidebar.info("‚öñÔ∏è **No Change**: Original forecasts (factor: 1.000x)")
    
    # Show examples for clarity
    with st.sidebar.expander("üí° Adjustment Examples"):
        st.write("""
        **Reduction Examples:**
        - -15% = 85% of original (0.85x)
        - -25% = 75% of original (0.75x)
        - -50% = 50% of original (0.50x)
        
        **Increase Examples:**
        - +10% = 110% of original (1.10x)
        - +25% = 125% of original (1.25x)
        - +50% = 150% of original (1.50x)
        
        **No Change:**
        - 0% = 100% of original (1.00x)
        """)
    
    # Display adjustment factor information
    if adjustment_percentage < 0:
        st.info(f"üìâ **Forecast Reduction Applied**: All predictions are automatically reduced by {abs(adjustment_percentage):.1f}% (adjustable in sidebar)")
    elif adjustment_percentage > 0:
        st.info(f"üìà **Forecast Increase Applied**: All predictions are automatically increased by {adjustment_percentage:.1f}% (adjustable in sidebar)")
    else:
        st.info("‚öñÔ∏è **No Adjustment Applied**: Forecasts show original predicted values (adjustable in sidebar)")

    # Advanced options
    st.sidebar.subheader("üî¨ Advanced Options")
    enable_hyperopt = st.sidebar.checkbox("Enable Hyperparameter Optimization", value=True, 
                                         help="Automatically tune model parameters for better accuracy")
    enable_meta_learning = st.sidebar.checkbox("Enable Meta-Learning", value=True,
                                              help="Use advanced stacking techniques")
    enable_preprocessing = st.sidebar.checkbox("Advanced Data Preprocessing", value=True,
                                              help="Outlier detection, transformation, and cleaning")

    # Model selection
    st.sidebar.subheader("ü§ñ Select Advanced Models")
    use_sarima = st.sidebar.checkbox("Advanced SARIMA (Auto-tuned)", value=True)
    use_prophet = st.sidebar.checkbox("Enhanced Prophet (Optimized)", value=True)
    use_ets = st.sidebar.checkbox("Auto-ETS (Best Config)", value=True)
    use_xgb = st.sidebar.checkbox("Advanced XGBoost (Feature-Rich)", value=True)

    if not any([use_sarima, use_prophet, use_ets, use_xgb]):
        st.sidebar.error("Please select at least one forecasting model.")
        return

    # File uploads
    st.subheader("üìÅ Upload Data Files")

    col1, col2 = st.columns(2)

    with col1:
        historical_file = st.file_uploader(
            "üìä Upload Historical Sales Data",
            type=["xlsx", "xls"],
            help="Excel file with 'Month' and 'Sales' columns - will be automatically preprocessed"
        )

    with col2:
        actual_2024_file = st.file_uploader(
            f"üìà Upload {forecast_year} Actual Data (Optional)",
            type=["xlsx", "xls"],
            help="For model validation, scaling detection, and meta-learning"
        )

    if historical_file is None:
        st.info("üëÜ Please upload historical sales data to begin advanced forecasting.")
        return

    # Load and validate historical data
    hist_df = load_data(historical_file)
    if hist_df is None:
        return

    # Load actual data for scaling detection and validation
    actual_2024_df = None
    scaling_factor = 1.0
    
    if actual_2024_file is not None:
        actual_2024_df = load_actual_2024_data(actual_2024_file, forecast_year)
        if actual_2024_df is not None:
            scaling_factor = detect_and_apply_scaling(hist_df, actual_2024_df)

    # Display enhanced data info
    st.subheader("üìä Advanced Data Analysis")

    # Calculate correct metrics based on unique months
    unique_months = hist_df['Month'].nunique()
    total_sales = hist_df['Sales'].sum()
    avg_monthly_sales = hist_df.groupby('Month')['Sales'].sum().mean()

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("üìÖ Total Months", unique_months)
    with col2:
        st.metric("üìà Avg Monthly Sales", f"{avg_monthly_sales:,.0f}")
    with col3:
        data_quality = min(100, unique_months * 4.17)
        st.metric("üéØ Data Quality Score", f"{data_quality:.0f}%")
    with col4:
        if 'log_transformed' in hist_df.columns and hist_df['log_transformed'].iloc[0]:
            st.metric("üîß Data Transformation", "Log Applied")
        else:
            st.metric("üîß Data Transformation", "None Applied")

    # Show additional data insights
    col1, col2 = st.columns(2)
    with col1:
        start_date = hist_df['Month'].min().strftime('%Y-%m')
        end_date = hist_df['Month'].max().strftime('%Y-%m')
        st.metric("üìÖ Data Range", f"{start_date} to {end_date}")
        
    with col2:
        total_rows = len(hist_df)
        if total_rows > unique_months:
            st.metric("üìä Data Points", f"{total_rows} rows ({unique_months} unique months)")
        else:
            st.metric("üìä Data Points", f"{total_rows}")

    # Show data breakdown if there are multiple entries per month
    if len(hist_df) > unique_months:
        avg_entries_per_month = len(hist_df) / unique_months
        st.info(f"üìä Your data contains multiple entries per month (avg: {avg_entries_per_month:.1f} entries/month). Sales are being aggregated by month for forecasting.")

    # Show seasonality and trend analysis
    col1, col2 = st.columns(2)
    with col1:
        monthly_data = hist_df.groupby('Month')['Sales'].sum().reset_index()
        if len(monthly_data) >= 24:
            try:
                if STATSMODELS_AVAILABLE:
                    decomposition = seasonal_decompose(monthly_data['Sales'], model='additive', period=12)
                    seasonal_strength = np.var(decomposition.seasonal) / np.var(monthly_data['Sales'])
                    st.metric("üìä Seasonality Strength", f"{seasonal_strength:.2%}")
                else:
                    st.metric("üìä Seasonality", "Analysis unavailable")
            except:
                st.metric("üìä Seasonality", "Analysis unavailable")
        else:
            st.metric("üìä Seasonality", "Need 24+ months")
        
    with col2:
        if len(monthly_data) >= 12:
            try:
                recent_trend = np.polyfit(range(len(monthly_data['Sales'].tail(12))), monthly_data['Sales'].tail(12), 1)[0]
                trend_direction = "üìà Increasing" if recent_trend > 0 else "üìâ Decreasing"
                st.metric("üìà Recent Trend", trend_direction)
            except:
                st.metric("üìà Recent Trend", "Analysis unavailable")
        else:
            st.metric("üìà Recent Trend", "Need 12+ months")

    # Show preprocessing results
    if enable_preprocessing and 'Sales_Original' in hist_df.columns:
        with st.expander("üîß Data Preprocessing Results"):
            col1, col2, col3 = st.columns(3)
            with col1:
                outliers_removed = (hist_df['Sales_Original'] != hist_df['Sales']).sum()
                st.metric("üéØ Outliers Handled", outliers_removed)
            with col2:
                if 'log_transformed' in hist_df.columns and hist_df['log_transformed'].iloc[0]:
                    st.info("üìä Log transformation applied to reduce skewness")
            with col3:
                st.metric("‚úÖ Data Points", len(hist_df))

    # Historical Sales Analysis Section
    st.subheader("üìà Historical Sales Analysis")
    st.markdown("**Year-over-year comparison of your actual sales performance**")
    
    # Create historical comparison chart
    historical_chart = create_historical_sales_comparison_chart(hist_df)
    if historical_chart:
        st.plotly_chart(historical_chart, use_container_width=True)
    
    # Show yearly summary table
    yearly_summary = create_yearly_summary_table(hist_df)
    if yearly_summary is not None:
        st.subheader("üìä Yearly Performance Summary")
        st.dataframe(yearly_summary, use_container_width=True)
        
        # Add insights
        if len(yearly_summary) > 1:
            col1, col2 = st.columns(2)
            with col1:
                # Best and worst performing years
                sales_col = 'Total Annual Sales'
                yearly_summary_numeric = yearly_summary.copy()
                yearly_summary_numeric['Sales_Numeric'] = yearly_summary_numeric[sales_col].str.replace(',', '').astype(float)
                
                best_year_idx = yearly_summary_numeric['Sales_Numeric'].idxmax()
                worst_year_idx = yearly_summary_numeric['Sales_Numeric'].idxmin()
                
                best_year = yearly_summary.iloc[best_year_idx]['Year']
                worst_year = yearly_summary.iloc[worst_year_idx]['Year']
                
                st.success(f"üèÜ **Best Year**: {best_year} ({yearly_summary.iloc[best_year_idx][sales_col]})")
                st.info(f"üìâ **Lowest Year**: {worst_year} ({yearly_summary.iloc[worst_year_idx][sales_col]})")
                
            with col2:
                # Growth trends
                growth_values = yearly_summary['YoY Growth (%)'].str.replace('%', '').str.replace('N/A', '').str.replace('+', '')
                growth_numeric = pd.to_numeric(growth_values, errors='coerce').dropna()
                
                if len(growth_numeric) > 0:
                    avg_growth = growth_numeric.mean()
                    if avg_growth > 0:
                        st.success(f"üìà **Avg YoY Growth**: +{avg_growth:.1f}%")
                    else:
                        st.warning(f"üìâ **Avg YoY Growth**: {avg_growth:.1f}%")
                    
                    # Consistency
                    growth_std = growth_numeric.std()
                    if growth_std < 10:
                        st.info("üéØ **Growth Pattern**: Consistent")
                    else:
                        st.info("‚ö° **Growth Pattern**: Variable")

    st.markdown("---")  # Separator line

    # Generate advanced forecasts
    if st.button("üöÄ Generate Advanced AI Forecasts", type="primary"):
        st.subheader("üöÄ Generating Advanced AI Forecasts...")
        
        # Show adjustment factor being applied
        if adjustment_percentage < 0:
            st.info(f"üìâ **Note**: All forecasts will be reduced by {abs(adjustment_percentage):.1f}% (multiplied by {current_adjustment_factor:.2f})")
        elif adjustment_percentage > 0:
            st.info(f"üìà **Note**: All forecasts will be increased by {adjustment_percentage:.1f}% (multiplied by {current_adjustment_factor:.2f})")
        else:
            st.info("‚öñÔ∏è **Note**: No adjustment will be applied to forecasts (multiplied by 1.00)")

        # Show optimization status
        if enable_hyperopt:
            st.info("üîß Hyperparameter optimization enabled - this may take longer but will improve accuracy")

        progress_bar = st.progress(0)
        forecast_results = {}
        validation_scores = {}

        # Create forecast dates
        forecast_dates = pd.date_range(
            start=f"{forecast_year}-01-01",
            end=f"{forecast_year}-12-01",
            freq='MS'
        )

        # Run each selected model with advanced features
        models_to_run = []
        if use_sarima:
            models_to_run.append(("SARIMA", run_advanced_sarima_forecast))
        if use_prophet:
            models_to_run.append(("Prophet", run_advanced_prophet_forecast))
        if use_ets:
            models_to_run.append(("ETS", run_advanced_ets_forecast))
        if use_xgb:
            models_to_run.append(("XGBoost", run_advanced_xgb_forecast))

        for i, (model_name, model_func) in enumerate(models_to_run):
            with st.spinner(f"ü§ñ Running advanced {model_name} with optimization..."):
                try:
                    # Run the model with error handling
                    result = model_func(hist_df, forecast_periods=12, scaling_factor=scaling_factor, adjustment_factor=current_adjustment_factor)
                    
                    if isinstance(result, tuple) and len(result) >= 2:
                        forecast_values, validation_score = result[0], result[1]
                    else:
                        forecast_values = result
                        validation_score = np.inf
                    
                    # Validate forecast values and fix any issues
                    if isinstance(forecast_values, (list, np.ndarray)):
                        forecast_values = np.array(forecast_values)
                        
                        # Ensure we have exactly 12 values
                        if len(forecast_values) != 12:
                            st.warning(f"‚ö†Ô∏è {model_name} returned {len(forecast_values)} values instead of 12. Using fallback.")
                            forecast_values = run_fallback_forecast(hist_df, forecast_periods=12, scaling_factor=scaling_factor, adjustment_factor=current_adjustment_factor)
                            forecast_values = apply_forecast_adjustment(forecast_values, current_adjustment_factor)
                            validation_score = np.inf
                        
                        # Check for valid forecasts
                        elif (np.all(forecast_values == 0) or 
                              np.any(np.isnan(forecast_values)) or 
                              np.any(np.isinf(forecast_values))):
                            st.warning(f"‚ö†Ô∏è {model_name} produced invalid forecast values. Using fallback.")
                            forecast_values = run_fallback_forecast(hist_df, forecast_periods=12, scaling_factor=scaling_factor, adjustment_factor=current_adjustment_factor)
                            forecast_values = apply_forecast_adjustment(forecast_values, current_adjustment_factor)
                            validation_score = np.inf
                        
                        # Store valid forecast
                        forecast_results[f"{model_name}_Forecast"] = forecast_values
                        validation_scores[model_name] = validation_score
                        
                        # Show forecast range for debugging
                        min_val, max_val = np.min(forecast_values), np.max(forecast_values)
                        score_text = f" (Range: {min_val:,.0f} - {max_val:,.0f})"
                        if validation_score != np.inf:
                            score_text += f" (Score: {validation_score:.2f})"
                        st.success(f"‚úÖ Advanced {model_name} completed{score_text}")
                        
                    else:
                        st.warning(f"‚ö†Ô∏è {model_name} returned invalid format. Using fallback.")
                        fallback_forecast = run_fallback_forecast(hist_df, forecast_periods=12, scaling_factor=scaling_factor, adjustment_factor=current_adjustment_factor)
                        fallback_forecast = apply_forecast_adjustment(fallback_forecast, current_adjustment_factor)
                        forecast_results[f"{model_name}_Forecast"] = fallback_forecast
                        validation_scores[model_name] = np.inf
                    
                except Exception as e:
                    st.error(f"‚ùå Advanced {model_name} failed: {str(e)}")
                    fallback_forecast = run_fallback_forecast(hist_df, forecast_periods=12, scaling_factor=scaling_factor, adjustment_factor=current_adjustment_factor)
                    fallback_forecast = apply_forecast_adjustment(fallback_forecast, current_adjustment_factor)
                    forecast_results[f"{model_name}_Forecast"] = fallback_forecast
                    validation_scores[model_name] = np.inf

            progress_bar.progress((i + 1) / len(models_to_run))

        # Validate that we have at least one successful forecast
        if not forecast_results:
            st.error("‚ùå All models failed. Please check your data and try again.")
            return

        # Create advanced ensemble
        if len(forecast_results) > 1:
            with st.spinner("üî• Creating intelligent weighted ensemble..."):
                try:
                    ensemble_values, ensemble_weights = create_weighted_ensemble(forecast_results, validation_scores)
                    forecast_results["Weighted_Ensemble"] = ensemble_values
                    
                    # Show ensemble weights
                    st.info(f"üéØ Ensemble weights: {', '.join([f'{k}: {v:.1%}' for k, v in ensemble_weights.items()])}")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Ensemble creation failed: {str(e)}")
        
        # Meta-learning ensemble
        if enable_meta_learning and actual_2024_df is not None:
            with st.spinner("üß† Training meta-learning model..."):
                try:
                    meta_forecast = run_meta_learning_forecast(forecast_results, actual_2024_df, forecast_periods=12)
                    if meta_forecast is not None:
                        forecast_results["Meta_Learning"] = meta_forecast
                        st.success("‚úÖ Meta-learning ensemble created successfully")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Meta-learning failed: {str(e)}")

        # Create results dataframe
        result_df = pd.DataFrame({
            "Month": forecast_dates,
            **forecast_results
        })

        # Merge actual data if available
        if actual_2024_df is not None:
            actual_2024_df['Month'] = pd.to_datetime(actual_2024_df['Month'])
            result_df['Month'] = pd.to_datetime(result_df['Month'])
            result_df = result_df.merge(actual_2024_df, on="Month", how="left")
            
            actual_count = result_df[f'Actual_{forecast_year}'].notna().sum()
            st.success(f"üìä Loaded {actual_count} months of actual data for validation")

        # Display results
        st.subheader("üìä Advanced Forecast Results")
        
        # Show adjustment impact
        if adjustment_percentage < 0:
            st.info(f"üìâ **Adjustment Applied**: All forecast values shown below have been reduced by {abs(adjustment_percentage):.1f}%")
        elif adjustment_percentage > 0:
            st.info(f"üìà **Adjustment Applied**: All forecast values shown below have been increased by {adjustment_percentage:.1f}%")
        else:
            st.info("‚öñÔ∏è **No Adjustment Applied**: Forecast values show original predictions")
        
        # Debug information - show forecast summaries
        if forecast_results:
            st.subheader("üîç Forecast Summary")
            debug_data = []
            for model_name, forecast_values in forecast_results.items():
                if isinstance(forecast_values, (list, np.ndarray)):
                    forecast_array = np.array(forecast_values)
                    debug_data.append({
                        'Model': model_name.replace('_Forecast', '').replace('_', ' '),
                        'Min Value': f"{np.min(forecast_array):,.0f}",
                        'Max Value': f"{np.max(forecast_array):,.0f}",
                        'Mean Value': f"{np.mean(forecast_array):,.0f}",
                        'Total Annual': f"{np.sum(forecast_array):,.0f}",
                        'Values Valid': "‚úÖ" if len(forecast_array) == 12 and not np.all(forecast_array == 0) else "‚ùå",
                        'Adjustment Applied': f"{adjustment_percentage:+.1f}%"
                    })
            
            if debug_data:
                debug_df = pd.DataFrame(debug_data)
                st.dataframe(debug_df, use_container_width=True)

        # Show forecast table with enhanced formatting
        display_df = result_df.copy()
        display_df['Month'] = display_df['Month'].dt.strftime('%Y-%m')
        
        for col in display_df.columns:
            if col != 'Month':
                display_df[col] = display_df[col].apply(lambda x: f"{x:,.0f}" if pd.notna(x) else "N/A")
        
        st.dataframe(display_df, use_container_width=True)

        # ADVANCED COMPARISON CHART
        st.subheader("üìä Advanced Model Performance Comparison")

        model_cols = [col for col in result_df.columns if '_Forecast' in col or col in ['Weighted_Ensemble', 'Meta_Learning']]
        actual_col = f'Actual_{forecast_year}'

        has_actual_data = actual_col in result_df.columns and result_df[actual_col].notna().any()

        if has_actual_data:
            # Get only months with actual data
            actual_data = result_df[result_df[actual_col].notna()].copy()
            
            # Show info about available data coverage
            available_months = actual_data['Month'].dt.strftime('%b %Y').tolist()
            st.info(f"üìÖ **Available actual data for {len(available_months)} months:** {', '.join(available_months)}")
            
            # Create improved comparison chart
            fig = create_comparison_chart_for_available_months_only(result_df, forecast_year, adjustment_percentage)
            
            if fig:
                st.plotly_chart(fig, use_container_width=True)
            
            # Advanced performance metrics - only for available months
            st.subheader("üéØ Advanced Performance Analysis")
            st.caption(f"Performance calculated only for months with actual data ({len(actual_data)} months)")
            
            if adjustment_percentage != 0:
                adj_text = f"reduced by {abs(adjustment_percentage):.1f}%" if adjustment_percentage < 0 else f"increased by {adjustment_percentage:.1f}%"
                st.warning(f"‚ö†Ô∏è **Note**: Forecasts have been {adj_text}, which may affect accuracy metrics when compared to original actual values.")
            
            performance_data = []
            actual_total = actual_data[actual_col].sum()
            
            for col in model_cols:
                model_name = col.replace('_Forecast', '').replace('_', ' ')
                
                # Calculate forecast total only for months with actual data
                forecast_total = actual_data[col].sum()
                
                # Calculate metrics only for available months
                metrics = calculate_accuracy_metrics(actual_data[actual_col], actual_data[col])
                if metrics:
                    bias = ((forecast_total - actual_total) / actual_total * 100) if actual_total > 0 else 0
                    
                    # Get validation score if available
                    val_score = validation_scores.get(model_name.replace(' ', ''), 'N/A')
                    val_score_text = f"{val_score:.2f}" if val_score != np.inf and val_score != 'N/A' else 'N/A'
                    
                    performance_data.append({
                        'Model': model_name,
                        'MAPE (%)': f"{metrics['MAPE']:.1f}%",
                        'SMAPE (%)': f"{metrics['SMAPE']:.1f}%",
                        'MAE': f"{metrics['MAE']:,.0f}",
                        'RMSE': f"{metrics['RMSE']:,.0f}",
                        'MASE': f"{metrics['MASE']:.2f}",
                        'Total Forecast (Available Months)': f"{forecast_total:,.0f}",
                        'Total Actual (Available Months)': f"{actual_total:,.0f}",
                        'Bias (%)': f"{bias:+.1f}%",
                        'Validation Score': val_score_text,
                        'Accuracy': f"{100 - metrics['MAPE']:.1f}%",
                        'Data Coverage': f"{len(actual_data)}/12 months",
                        'Adjustment Applied': f"{adjustment_percentage:+.1f}%"
                    })
            
            if performance_data:
                performance_df = pd.DataFrame(performance_data)
                st.dataframe(performance_df, use_container_width=True)
                
                # Show best performing model
                best_model = performance_df.loc[performance_df['MAPE (%)'].str.replace('%', '').astype(float).idxmin()]
                st.success(f"üèÜ Best performing model: **{best_model['Model']}** with {best_model['MAPE (%)']} MAPE")
                
                # Show data coverage info
                coverage_pct = len(actual_data) / 12 * 100
                if coverage_pct < 100:
                    st.warning(f"‚ö†Ô∏è Performance analysis based on {len(actual_data)} months of actual data ({coverage_pct:.0f}% coverage)")

        else:
            # Forecast-only view
            st.warning("üìä No actual data for validation. Showing advanced forecasts.")
            
            fig = go.Figure()
            colors = ['#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#FF9F43', '#6C5CE7']
            
            for i, col in enumerate(model_cols):
                if col in ['Weighted_Ensemble', 'Meta_Learning']:
                    line_style = dict(color='#6C5CE7', width=3, dash='dash') if col == 'Weighted_Ensemble' else dict(color='#00D2D3', width=3, dash='dot')
                    icon = 'üî•' if col == 'Weighted_Ensemble' else 'üß†'
                else:
                    line_style = dict(color=colors[i % len(colors)], width=2)
                    icon = 'üìà'
                
                model_name = col.replace('_Forecast', '').replace('_', ' ').upper()
                fig.add_trace(go.Scatter(
                    x=result_df['Month'],
                    y=result_df[col],
                    mode='lines+markers',
                    name=f'{icon} {model_name}',
                    line=line_style,
                    marker=dict(size=6)
                ))
            
            # Create dynamic title based on adjustment
            if adjustment_percentage < 0:
                adj_text = f"{abs(adjustment_percentage):.1f}% Reduction Applied"
            elif adjustment_percentage > 0:
                adj_text = f"{adjustment_percentage:.1f}% Increase Applied"
            else:
                adj_text = "No Adjustment Applied"
            
            fig.update_layout(
                title=f'üöÄ ADVANCED AI FORECAST MODELS COMPARISON ({adj_text})',
                xaxis_title='Month',
                yaxis_title='Sales Volume',
                height=700,
                hovermode='x unified'
            )
            
            st.plotly_chart(fig, use_container_width=True)

        # Summary
        st.subheader("üéØ Advanced Forecast Summary")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if 'Weighted_Ensemble' in result_df.columns:
                ensemble_total = result_df['Weighted_Ensemble'].sum()
                original_ensemble_total = ensemble_total / current_adjustment_factor if current_adjustment_factor != 0 else ensemble_total
                st.metric("üî• Intelligent Ensemble", f"{ensemble_total:,.0f}", 
                         delta=f"Original: {original_ensemble_total:,.0f}")
        
        with col2:
            if 'Meta_Learning' in result_df.columns:
                meta_total = result_df['Meta_Learning'].sum()
                original_meta_total = meta_total / current_adjustment_factor if current_adjustment_factor != 0 else meta_total
                st.metric("üß† Meta-Learning", f"{meta_total:,.0f}",
                         delta=f"Original: {original_meta_total:,.0f}")
        
        with col3:
            successful_models = len([v for v in validation_scores.values() if v != np.inf])
            total_models = len(validation_scores)
            st.metric("ü§ñ Models Successful", f"{successful_models}/{total_models}")
        
        with col4:
            combined_factor = scaling_factor * current_adjustment_factor
            st.metric("üìä Combined Factor", f"{combined_factor:.3f}x")

        # Download options
        st.subheader("üìä Download Results")
        
        col1, col2 = st.columns(2)
        
        with col1:
            csv = result_df.to_csv(index=False).encode("utf-8")
            adj_text = f"adj_{adjustment_percentage:+.1f}pct" if adjustment_percentage != 0 else "no_adj"
            st.download_button(
                label="üìÑ Download CSV Data",
                data=csv,
                file_name=f"advanced_forecasts_{forecast_year}_{adj_text}.csv",
                mime="text/csv"
            )
        
        with col2:
            st.info("üìä CSV contains all forecast results with adjustments applied")

    # ADDITIONAL HISTORICAL ANALYSIS CHART AT BOTTOM
    st.markdown("---")
    st.subheader("üìä Complete Historical Sales Overview")
    st.markdown("**Complete view of your historical sales data across all years for reference**")
    
    # Create a comprehensive historical chart
    historical_overview = create_historical_sales_comparison_chart(hist_df)
    if historical_overview:
        st.plotly_chart(historical_overview, use_container_width=True)
        
        # Add some context about the historical data
        work_data = hist_df.copy()
        work_data['Year'] = pd.to_datetime(work_data['Month']).dt.year
        sales_col = 'Sales_Original' if 'Sales_Original' in work_data.columns else 'Sales'
        
        # Calculate some overall statistics
        years_span = work_data['Year'].max() - work_data['Year'].min() + 1
        total_historical_sales = work_data.groupby('Month')[sales_col].sum().sum()
        avg_annual_sales = total_historical_sales / years_span
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("üìÖ Years of Data", f"{years_span} years")
        with col2:
            st.metric("üí∞ Total Historical Sales", f"{total_historical_sales:,.0f}")
        with col3:
            st.metric("üìä Avg Annual Sales", f"{avg_annual_sales:,.0f}")
        
        st.info("üí° **Tip**: Use this historical overview to understand seasonal patterns and growth trends that inform your forecasting strategy.")


if __name__ == "__main__":
    main()
